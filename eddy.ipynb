{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model \n",
    "\n",
    "Making good Eddies prediction. You can train your one model on colab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First you need to download the files, and put them on the data folder. \n",
    "!git clone https://github.com/guillaume117/EddyForecasting.git\n",
    "%cd EddyForecasting\n",
    "!pip install gdown\n",
    "!pip install netCDF4\n",
    "\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "\n",
    "\n",
    "\n",
    "import gdown\n",
    "import os\n",
    "from  utils.util import addGitignore\n",
    "path = 'data'\n",
    "addGitignore(path)\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "else:\n",
    "    pass\n",
    "pass\n",
    "\n",
    "\n",
    "file_url_train = \"https://drive.google.com/uc?export=download&id=1RxEA59SUTbniBJrIO4Twa7w7de-QlIiH\"\n",
    "file_url_test = \"https://drive.google.com/uc?export=download&id=1hWUzgu5UrjQNF4T4GVfX_g7jPD_HXJfg\"\n",
    "file_url_target = \"https://drive.google.com/uc?export=download&id=1zNT1WLF3Fbsu3LEaEQxPhqPgUO7eQbDs\"\n",
    "\n",
    "gdown.download(file_url_train, output=f'{path}/OSSE_U_V_SLA_SST_2015_TRAIN_for_FORECAST.nc', quiet=False)\n",
    "gdown.download(file_url_test, output=f'{path}/OSSE_U_V_SLA_SST_2015_TEST_for_FORECAST.nc', quiet=False)\n",
    "gdown.download(file_url_target, output=f'{path}/eddies_TRAIN_for_FORECAST.nc', quiet=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from CNN.unet import UNet\n",
    "from utils.train import TrainDataset\n",
    "from utils.generate import GenerateDataset\n",
    "from utils.device import self_device\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "#Generate dataset is a custom class that is adapted to the specific needs of the project. \n",
    "#It is used to generate the dataset from the raw data, and to record the datasets in a specific folder dataset/running_instance\n",
    "\n",
    "#Choose the name of the running instance, it will be used to save the dataset\n",
    "running_instance = 'my_instance'\n",
    "dataset = GenerateDataset()\n",
    "dataset.processingData(num_date=10,\n",
    "                    type ='Train', #Train or Test, Test will be only used for scoring\n",
    "                    running_instance = 'my_instance', #The name of the running instance, will be used to save the dataset\n",
    "                    generate =True, #If True, the dataset will be generated, if False, the dataset will be loaded\n",
    "                    validation_fraction=0.2 #The fraction of the dataset that will be used for validation\n",
    "                    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_path = dataset.getTrainPath()\n",
    "val_path = dataset.getValPath()\n",
    "\n",
    "\n",
    "nan_path = dataset.getNanMaskLabelPath()\n",
    "nan_deconv_path = dataset.getNanMaskLabelDeconvPath()\n",
    "\n",
    "del dataset\n",
    "#The dataset is saved in the dataset/running_instance folder, and can be loaded using the following command\n",
    "train_dataset = torch.load(train_path)\n",
    "val_dataset = torch.load(val_path)\n",
    "\n",
    "#The nan mask are used to mask the nan values in the dataset, and are used to compute the loss. \n",
    "#The loss is not computed for the nan values\n",
    "nan_masked_label = torch.load(nan_path)\n",
    "\n",
    "#The nan mask deconv is not necessarly well named, it is the nan mask with the original shape of the dataset. It will be used \n",
    "#later to compute the score of the model. \n",
    "nan_masked_label_deconv = torch.load(nan_deconv_path)\n",
    "\n",
    "#The device is used to define if the model will be trained on the GPU which can be Cuda, Metal or CPU, or the CPU if no GPU.\n",
    "#This parameters is sent to the train, since if METAL MPS is used, tensors must be converted to float32\n",
    "device = self_device()\n",
    "print(f'GPU device type = {device}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = UNet(n_class=3,#3 classes, 0: no eddies layer, 1: cyclone, 2: anticyclone\n",
    "             num_dates = 10, #The number of dates to be forecasted, must be the same as the number of dates in the dataset\n",
    "             verbose=False\n",
    "             )\n",
    "#if you want to load a trained model, you can use \n",
    "#weights_path = 'UNet_trained/XXx.pth'\n",
    "#model.load_state_dict(torch.load(weights_path))\n",
    "#In this case, please take care not to generate another dataset, \n",
    "#since the dataset will be different from the one used to train the model\n",
    "\n",
    "model.to(device)\n",
    "num_epochs =100\n",
    "learning_rate=0.01\n",
    "batch_size = 10\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), betas=(0.9, 0.99))\n",
    "scheduler =  lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.98)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "TD = TrainDataset(model=model,\n",
    "                  criterion=criterion,  \n",
    "                  optimizer=optimizer,\n",
    "                  scheduler=scheduler,\n",
    "                  num_epochs=num_epochs,\n",
    "                  learning_rate=learning_rate,\n",
    "                  batch_size=batch_size,\n",
    "                  device=device,\n",
    "                  nan_mask_label=nan_masked_label[:batch_size,:,:,:],\n",
    "                  running_instance=running_instance,\n",
    "                  num_dates=10)\n",
    "\n",
    "\n",
    "model = TD.train_model(train_dataset,val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This part of the code is needed to load the validation dataset and get the input and labels\"\"\"\n",
    "from torch.utils.data import DataLoader\n",
    "val_dataset = torch.load(val_path)\n",
    "\n",
    "val_loader = DataLoader(dataset = val_dataset, batch_size=len(val_dataset),shuffle=False, drop_last=False)\n",
    "for input, labels in val_loader:\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This is the code to load the trained model and the weights and to make the prediction. \n",
    "The prediction is then reshaped to the original shape of the image.\n",
    "The prediction is then converted to a one hot encoding and then to a label.\n",
    "The label is then reshaped to the original shape of the image.\n",
    "The label and the prediction are then saved to a list.\n",
    "\n",
    "    \"\"\"\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "list_following_predicted = []\n",
    "list_following_ground_truth=[]\n",
    "model = UNet(n_class=3,num_dates = 10, verbose=False)\n",
    "weights_path = 'UNet_trained/UNet_Train_1_Epoch_98_valacc_91.76040649414062.pth'\n",
    "model.load_state_dict(torch.load(weights_path))\n",
    "model.to('cpu')\n",
    "model.eval()\n",
    "\n",
    "for i in tqdm(range(len(input))): \n",
    "       \n",
    "    item= input[i].unsqueeze(0)\n",
    "    label = labels[i].unsqueeze(0)\n",
    "    item =model(item)\n",
    "    trans_back = transforms.Resize((357,717))\n",
    "    item = trans_back(item)\n",
    "    label = trans_back(label)\n",
    "\n",
    "    reshaped_output = item.view(item.size(0), 3, 10, item.size(2), item.size(3))\n",
    "    max_indices = torch.argmax(reshaped_output, dim=1)\n",
    "    one_hot_encoding = torch.zeros_like(reshaped_output)\n",
    "    item = one_hot_encoding.scatter_(1, max_indices.unsqueeze(1), 1)\n",
    "    item =0*item[:,0,:,:,:]+1*item[:,1,:,:,:]+2*item[:,2,:,:,:]\n",
    "    item = torch.where(nan_masked_label_deconv[0,:,:,:],torch.tensor(999),item)\n",
    "\n",
    "\n",
    "    reshaped_label = label.view(label.size(0), 3, 10, label.size(2), label.size(3))\n",
    "    max_indices_label = torch.argmax(reshaped_label, dim=1)\n",
    "    one_hot_encoding_label = torch.zeros_like(reshaped_label)\n",
    "    label= one_hot_encoding_label.scatter_(1, max_indices_label.unsqueeze(1), 1)\n",
    "    label=0*label[:,0,:,:,:]+1*label[:,1,:,:,:]+2*label[:,2,:,:,:]\n",
    "    label = torch.where(nan_masked_label_deconv[0,:,:,:],torch.tensor(999),label)\n",
    "\n",
    "\n",
    "    label=label.squeeze(0)\n",
    "    label_final = label[:,:,:].detach().numpy()\n",
    "    list_following_ground_truth.append(label_final)\n",
    "\n",
    "    item=item.squeeze(0)\n",
    "    item_final = item[:,:,:].detach().numpy()\n",
    "    list_following_predicted.append(item_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eddies animation \n",
    "\n",
    "<div class ='alert alert-success'>\n",
    "\n",
    "The two next following cells are for creating video animation of prediction vs ground Truth\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "\"\"\"\n",
    "This is the code to create a video of the prediction and the ground truth\n",
    "It takes the prediction and the ground truth and reshapes them to the original shape of the image.\n",
    "It then creates a video of the prediction and the ground truth.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "array_list_following_predicted=np.array(list_following_predicted)\n",
    "#we don't want to saturate the image with the land(=999) so we set the land to 0.5\n",
    "array_list_following_predicted=np.where(array_list_following_predicted>=100,0.5,array_list_following_predicted)\n",
    "array_list_following_ground_truth=np.array(list_following_ground_truth)\n",
    "array_list_following_ground_truth=np.where(array_list_following_ground_truth>=100,0.5,array_list_following_ground_truth)\n",
    "fig, ax = plt.subplots(1, 2, figsize = (40, 32))\n",
    "def init():\n",
    "   \n",
    "    ax[0].imshow(array_list_following_predicted[10][0],origin=\"lower\")\n",
    "    ax[1].imshow(array_list_following_ground_truth[10][0],origin=\"lower\")\n",
    " \n",
    "    return [ax]\n",
    "\n",
    "def update(frame):\n",
    "    ax[0].imshow(array_list_following_predicted[10][frame],origin=\"lower\")\n",
    "    ax[1].imshow(array_list_following_ground_truth[10][frame],origin=\"lower\")\n",
    "\n",
    "    return [ax]\n",
    "\n",
    "\n",
    "animation = FuncAnimation(fig, update, frames=len(array_list_following_predicted[1]), init_func=init)\n",
    "\n",
    "output_video = 'predicted_video_eddies_predict_vs_ground_truth.mp4'\n",
    "\n",
    "animation.save(output_video, fps=1, extra_args=['-vcodec', 'libx264'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention ne pas supprimer cette cellule :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt2\n",
    "\n",
    "def confusion_matrix_(gT_label, pred_label):\n",
    "    \"\"\" This function takes the ground truth and the prediction and computes the confusion matrix\"\"\"\n",
    "    \n",
    "    gT_label = np.array(gT_label).flatten()\n",
    "    pred_label = np.array(pred_label).flatten()\n",
    "    cm = confusion_matrix(gT_label,pred_label,labels=[0,1,2],normalize='true')\n",
    "    return cm\n",
    "\n",
    "\n",
    "cm = confusion_matrix_(array_list_following_ground_truth,array_list_following_predicted)\n",
    "ax = seaborn.heatmap(100*cm,annot=True, cmap=\"crest\",linewidths=0.05)\n",
    "ax.set(xlabel=\"Ground Truth \", ylabel=\"Prediction\")\n",
    "ax.xaxis.tick_top()\n",
    "plt2.show()\n",
    "\n",
    "print(100*cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SDD_Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
